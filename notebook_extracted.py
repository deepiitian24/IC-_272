# Auto-generated from notebook: Assignment_2_IC272 (1).ipynb
# Do not edit this file directly; edit the notebook instead.


# ==== Notebook code cell 1 ====
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

# ==== Notebook code cell 2 ====
np.random.seed(42)

# ==== Notebook code cell 3 ====
from pathlib import Path
import os

def _find_dataset_path() -> str | None:
    env_path = os.getenv('COMPANY_DATA_CSV')
    if env_path and Path(env_path).exists():
        return env_path

    # Exact filename anywhere
    exact = list(Path('.').rglob('Company_Data.csv'))
    if exact:
        return str(exact[0])

    # Case-insensitive search for files containing 'company' and 'data'
    ci = []
    for p in Path('.').rglob('*.csv'):
        name = p.name.lower()
        if ('company' in name) and ('data' in name):
            ci.append(p)
    if ci:
        # pick the first one
        return str(ci[0])

    # Fallback to root
    fallback = Path('./Company_Data.csv')
    return str(fallback) if fallback.exists() else None

_csv_path = _find_dataset_path()
print(f"Working directory: {os.getcwd()}")
if _csv_path is None:
    print("ERROR: Could not find 'Company_Data.csv'. Set env COMPANY_DATA_CSV=/absolute/path/to/Company_Data.csv.")
    # Listing available CSVs for convenience
    found = [str(p) for p in Path('.').rglob('*.csv')]
    print(f"Found CSV files: {found}")
    raise SystemExit(2)
print(f"Loading dataset from: {_csv_path}")
df=pd.read_csv(_csv_path)

# ==== Notebook code cell 4 ====
print(df.info())
print(df.describe())

# ==== Notebook code cell 5 ====
df.head()

# ==== Notebook code cell 6 ====
df['Sales_Value'] = np.where(df['Sales'] > 8, 1, 0)

# ==== Notebook code cell 7 ====
df.drop(columns='Sales',inplace=True)

# ==== Notebook code cell 8 ====
df

# ==== Notebook code cell 9 ====
df['ShelveLoc']=df['ShelveLoc'].replace({'Good':2,'Medium':1,'Bad':0})

# ==== Notebook code cell 10 ====
df['Urban']=df['Urban'].replace({'Yes':1,'No':0})

# ==== Notebook code cell 11 ====
df['US']=df['US'].replace({'Yes':1,'No':0})

# ==== Notebook code cell 12 ====
X=df.iloc[:,:-1]
y=df.iloc[:,-1]

# ==== Notebook code cell 13 ====
# Custom Train-Test Split
def train_test_split_np(X, y, test_size=0.2, random_state=None):
    """
    Train-Test Split
    """
    X_is_df = isinstance(X, pd.DataFrame)
    y_is_ser = isinstance(y, pd.Series)

    n_samples = X.shape[0]
    rng = np.random.default_rng(random_state)
    indices = np.arange(n_samples)
    rng.shuffle(indices)

    test_size_int = int(n_samples * test_size)
    test_idx = indices[:test_size_int]
    train_idx = indices[test_size_int:]

    if X_is_df:
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    else:
        X_train, X_test = X[train_idx], X[test_idx]

    if y_is_ser:
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    else:
        y_train, y_test = y[train_idx], y[test_idx]

    return X_train, X_test, y_train, y_test

# ==== Notebook code cell 14 ====
# Train / Validation / Test Split (70/15/15)
X_train, X_temp, y_train, y_temp = train_test_split_np(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split_np(X_temp, y_temp, test_size=0.5, random_state=42)

# ==== Notebook code cell 15 ====
class Node():
    """
    A class representing a node in a decision tree.
    """

    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):
        """
        Initializes a new instance of the Node class.

        Args:
            feature: The feature used for splitting at this node. Defaults to None.
            threshold: The threshold used for splitting at this node. Defaults to None.
            left: The left child node. Defaults to None.
            right: The right child node. Defaults to None.
            gain: The gain of the split. Defaults to None.
            value: If this node is a leaf node, this attribute represents the predicted value
                for the target variable. Defaults to None.
        """
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.gain = gain
        self.value = value

# ==== Notebook code cell 16 ====
class DecisionTree():
    """
    A decision tree classifier for binary classification problems.
    """

    def __init__(self, min_samples=2, max_depth=2, max_features_per_split=None, random_state=None):
        """
        Constructor for DecisionTree class.

        Parameters:
            min_samples (int): Minimum number of samples required to split an internal node.
            max_depth (int): Maximum depth of the decision tree.
        """
        self.min_samples = min_samples
        self.max_depth = max_depth
        self.max_features_per_split = max_features_per_split
        self.rng = np.random.default_rng(random_state)
        # empty dictionary to store feature importance
        self.feature_importance = {}


    def split_data(self, dataset, feature, threshold):
        """
        Splits the given dataset into two datasets based on the given feature and threshold.

        Parameters:
            dataset (ndarray): Input dataset.
            feature (int): Index of the feature to be split on.
            threshold (float): Threshold value to split the feature on.

        Returns:
            left_dataset (ndarray): Subset of the dataset with values less than or equal to the threshold.
            right_dataset (ndarray): Subset of the dataset with values greater than the threshold.
        """
        # Create empty arrays to store the left and right datasets
        left_dataset = []
        right_dataset = []

        # Loop over each row in the dataset and split based on the given feature and threshold
        for row in dataset:
            if row[feature] <= threshold:
                left_dataset.append(row)
            else:
                right_dataset.append(row)

        # Convert the left and right datasets to numpy arrays and return
        left_dataset = np.array(left_dataset)
        right_dataset = np.array(right_dataset)
        return left_dataset, right_dataset

    def entropy(self, y):
        """
        Computes the entropy of the given label values.

        Parameters:
            y (ndarray): Input label values.

        Returns:
            entropy (float): Entropy of the given label values.
        """
        entropy = 0

        # Find the unique label values in y and loop over each value
        labels = np.unique(y)
        for label in labels:
            # Find the examples in y that have the current label
            label_examples = y[y == label]
            # Calculate the ratio of the current label in y
            pl = len(label_examples) / len(y)
            # Calculate the entropy using the current label and ratio
            entropy += -pl * np.log2(pl)

        # Return the final entropy value
        return entropy

    def information_gain(self, parent, left, right):
        """
        Computes the information gain from splitting the parent dataset into two datasets.

        Parameters:
            parent (ndarray): Input parent dataset.
            left (ndarray): Subset of the parent dataset after split on a feature.
            right (ndarray): Subset of the parent dataset after split on a feature.

        Returns:
            information_gain (float): Information gain of the split.
        """
        # set initial information gain to 0
        information_gain = 0
        # compute entropy for parent
        parent_entropy = self.entropy(parent)
        # calculate weight for left and right nodes
        weight_left = len(left) / len(parent)
        weight_right= len(right) / len(parent)
        # compute entropy for left and right nodes
        entropy_left, entropy_right = self.entropy(left), self.entropy(right)
        # calculate weighted entropy
        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right
        # calculate information gain
        information_gain = parent_entropy - weighted_entropy
        return information_gain


    def _resolve_max_features(self, num_features: int) -> int:
        mf = self.max_features_per_split
        if mf is None:
            return num_features
        if isinstance(mf, str):
            mf_l = mf.lower()
            if mf_l == "sqrt":
                return max(1, int(math.sqrt(num_features)))
            if mf_l == "log2":
                return max(1, int(math.log2(num_features)))
            return num_features
        if isinstance(mf, float):
            return max(1, int(num_features * mf))
        if isinstance(mf, int):
            return max(1, min(num_features, mf))
        return num_features

    def best_split(self, dataset, num_samples, num_features):
        """
        Finds the best split for the given dataset.

        Args:
        dataset (ndarray): The dataset to split.
        num_samples (int): The number of samples in the dataset.
        num_features (int): The number of features in the dataset.

        Returns:
        dict: A dictionary with the best split feature index, threshold, gain,
              left and right datasets.
        """
        # dictionary to store the best split values
        best_split = {'gain':- 1, 'feature': None, 'threshold': None}
        # choose subset of features for this split (if enabled)
        k_features = self._resolve_max_features(num_features)
        if k_features < num_features:
            feature_indices_iter = self.rng.choice(num_features, size=k_features, replace=False)
        else:
            feature_indices_iter = range(num_features)
        # loop over chosen features
        for feature_index in feature_indices_iter:
            #get the feature at the current feature_index
            feature_values = dataset[:, feature_index]
            #get unique values of that feature
            thresholds = np.unique(feature_values)
            # loop over all values of the feature
            for threshold in thresholds:
                # get left and right datasets
                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)
                # check if either datasets is empty
                if len(left_dataset) and len(right_dataset):
                    # get y values of the parent and left, right nodes
                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]
                    # compute information gain based on the y values
                    information_gain = self.information_gain(y, left_y, right_y)
                    # update the best split if conditions are met
                    if information_gain > best_split["gain"]:
                        best_split["feature"] = feature_index
                        best_split["threshold"] = threshold
                        best_split["left_dataset"] = left_dataset
                        best_split["right_dataset"] = right_dataset
                        best_split["gain"] = information_gain
        return best_split


    def calculate_leaf_value(self, y):
        """
        Calculates the most occurring value in the given list of y values.

        Args:
            y (list): The list of y values.

        Returns:
            The most occurring value in the list.
        """
        y = list(y)
        #get the highest present class in the array
        most_occuring_value = max(y, key=y.count)
        return most_occuring_value

    def build_tree(self, dataset, current_depth=0):#This is a recursive function that actually builds the full tree.
        """
        Recursively builds a decision tree from the given dataset.

        Args:
        dataset (ndarray): The dataset to build the tree from.
        current_depth (int): The current depth of the tree.

        Returns:
        Node: The root node of the built decision tree.
        """
        # split the dataset into X, y values
        X, y = dataset[:, :-1], dataset[:, -1]
        n_samples, n_features = X.shape
        # keeps spliting until stopping conditions are met
        if n_samples >= self.min_samples and (self.max_depth is None or current_depth <= self.max_depth):
            # Get the best split
            best_split = self.best_split(dataset, n_samples, n_features)
            # Check if gain isn't zero
            if best_split["gain"]:
              # record importance for the feature
                feature_index = best_split["feature"]
                gain_value = best_split["gain"]
                if feature_index in self.feature_importance:
                  self.feature_importance[feature_index] += gain_value
                else:
                  self.feature_importance[feature_index] = gain_value
            # continue splitting recursively
                left_node = self.build_tree(best_split["left_dataset"], current_depth + 1)
                right_node = self.build_tree(best_split["right_dataset"], current_depth + 1)
                return Node(best_split["feature"], best_split["threshold"],
                            left_node, right_node, best_split["gain"])

        # compute leaf node value
        leaf_value = self.calculate_leaf_value(y)
        # return leaf node value
        return Node(value=leaf_value)

    def fit(self, X, y):
        """
        Builds and fits the decision tree to the given X and y values.

        Args:
        X (ndarray): The feature matrix.
        y (ndarray): The target values.
        """
        # Reshape y to be 2-dimensional before concatenation
        y_reshaped = y.reshape(-1, 1)
        dataset = np.concatenate((X, y_reshaped), axis=1)
        self.root = self.build_tree(dataset)
        # normalize feature importance so they sum to 1
        total_gain = sum(self.feature_importance.values())
        if total_gain != 0:
            for f in self.feature_importance:
                self.feature_importance[f] = self.feature_importance[f] / total_gain


    def predict(self, X):
        """
        Predicts the class labels for each instance in the feature matrix X.

        Args:
        X (ndarray): The feature matrix to make predictions for.

        Returns:
        list: A list of predicted class labels.
        """
        # Create an empty list to store the predictions
        predictions = []
        # For each instance in X, make a prediction by traversing the tree
        for x in X:
            prediction = self.make_prediction(x, self.root)
            # Append the prediction to the list of predictions
            predictions.append(prediction)
        # Convert the list to a numpy array and return it
        np.array(predictions)
        return predictions

    def make_prediction(self, x, node):
        """
        Traverses the decision tree to predict the target value for the given feature vector.

        Args:
        x (ndarray): The feature vector to predict the target value for.
        node (Node): The current node being evaluated.

        Returns:
        The predicted target value for the given feature vector.
        """
        # if the node has value i.e it's a leaf node extract it's value
        if node.value != None:
            return node.value
        else:
            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly
            feature = x[node.feature]
            if feature <= node.threshold:
                return self.make_prediction(x, node.left)
            else:
                return self.make_prediction(x, node.right)

# ==== Notebook code cell 16.1 (helpers for post-pruning) ====
def _postorder_nodes(node):
    nodes = []
    def visit(n):
        if n is None:
            return
        if n.value is None:
            visit(n.left)
            visit(n.right)
        nodes.append(n)
    visit(node)
    return nodes

def _collect_train_indices_per_node(node, X: np.ndarray, indices: np.ndarray, mapping: dict):
    mapping[id(node)] = indices
    if node.value is None and indices.size > 0:
        left_mask = X[indices, node.feature] <= node.threshold
        left_idx = indices[left_mask]
        right_idx = indices[~left_mask]
        _collect_train_indices_per_node(node.left, X, left_idx, mapping)
        _collect_train_indices_per_node(node.right, X, right_idx, mapping)

def reduced_error_prune(model: DecisionTree,
                        X_train: np.ndarray,
                        y_train: np.ndarray,
                        X_val: np.ndarray,
                        y_val: np.ndarray) -> tuple[int, float]:
    """
    Perform Reduced Error Post-Pruning on a fully grown tree using validation set.

    Returns (num_pruned_nodes, best_val_accuracy_after_pruning)
    """
    # Baseline validation accuracy
    def validate_acc() -> float:
        preds = np.array(model.predict(X_val), dtype=int)
        return float(np.sum(preds == y_val.flatten()) / y_val.shape[0])

    best_val_acc = validate_acc()
    total_pruned = 0

    # Precompute training indices per node once from the original tree
    node_to_indices: dict[int, np.ndarray] = {}
    _collect_train_indices_per_node(model.root, X_train, np.arange(X_train.shape[0]), node_to_indices)

    while True:
        pruned_this_pass = 0
        nodes = _postorder_nodes(model.root)
        for node in nodes:
            if node is None or node.value is not None:
                continue  # skip leaves
            # Backup
            backup = (node.feature, node.threshold, node.left, node.right, node.gain, node.value)
            # Majority class at this node from training samples
            idx = node_to_indices.get(id(node), np.array([], dtype=int))
            if idx.size == 0:
                majority_value = 0 if np.mean(y_train) < 0.5 else 1
            else:
                majority_value = model.calculate_leaf_value(y_train[idx])
            # Prune: make it a leaf
            node.feature = None
            node.threshold = None
            node.left = None
            node.right = None
            node.gain = None
            node.value = majority_value
            # Evaluate
            new_val_acc = validate_acc()
            if new_val_acc + 1e-12 >= best_val_acc:
                best_val_acc = new_val_acc
                pruned_this_pass += 1
            else:
                # Revert
                node.feature, node.threshold, node.left, node.right, node.gain, node.value = backup
        if pruned_this_pass == 0:
            break
        total_pruned += pruned_this_pass
    return total_pruned, best_val_acc

# ==== Notebook code cell 17 (added) ====
class RandomForest:
    """
    Random Forest classifier built on the custom DecisionTree above.

    Uses bootstrap sampling and feature subsampling (max_features) per tree.
    """

    def __init__(
        self,
        n_estimators: int = 100,
        max_depth: int | None = None,
        min_samples: int = 2,
        max_features: int | float | str | None = "sqrt",
        bootstrap: bool = True,
        random_state: int | None = None,
    ) -> None:
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.random_state = random_state

        self.estimators_: list[DecisionTree] = []
        self.feature_indices_per_estimator_: list[np.ndarray] = []
        # Map of original feature index -> normalized importance
        self.feature_importances_: dict[int, float] = {}

    def _resolve_max_features(self, num_features: int) -> int:
        mf = self.max_features
        if mf is None:
            return num_features
        if isinstance(mf, str):
            if mf.lower() == "sqrt":
                return max(1, int(math.sqrt(num_features)))
            if mf.lower() == "log2":
                return max(1, int(math.log2(num_features)))
            return num_features
        if isinstance(mf, float):
            # treat as fraction
            return max(1, int(num_features * mf))
        if isinstance(mf, int):
            return max(1, min(num_features, mf))
        return num_features

    def _bootstrap_indices(self, n_samples: int, rng: np.random.Generator) -> np.ndarray:
        return rng.integers(0, n_samples, size=n_samples)

    def fit(self, X: np.ndarray, y: np.ndarray) -> "RandomForest":
        rng = np.random.default_rng(self.random_state)
        self.estimators_.clear()
        self.feature_indices_per_estimator_.clear()
        aggregated_importance: dict[int, float] = {}

        n_samples, num_features = X.shape
        k_features = self._resolve_max_features(num_features)

        # Track OOB votes across trees
        oob_vote_sum = np.zeros(n_samples, dtype=float)
        oob_vote_count = np.zeros(n_samples, dtype=int)
        self.oob_error_curve_: list[float] = []

        for i in range(self.n_estimators):
            # Bootstrap sampling (by indices)
            if self.bootstrap:
                boot_indices = self._bootstrap_indices(n_samples, rng)
            else:
                boot_indices = np.arange(n_samples)

            oob_mask = np.ones(n_samples, dtype=bool)
            oob_mask[boot_indices] = False

            # Feature subsampling for the whole tree input space
            feature_indices = rng.choice(num_features, size=k_features, replace=False)
            feature_indices = np.array(sorted(feature_indices))

            # Train a DecisionTree on the subset of features with per-split sampling enabled
            X_boot_subset = X[boot_indices][:, feature_indices]
            y_boot_subset = y[boot_indices]
            tree = DecisionTree(
                min_samples=self.min_samples,
                max_depth=self.max_depth,
                max_features_per_split=self.max_features,
                random_state=(None if self.random_state is None else self.random_state + i),
            )
            tree.fit(X_boot_subset, y_boot_subset)

            self.estimators_.append(tree)
            self.feature_indices_per_estimator_.append(feature_indices)

            # Map tree-specific feature importances back to original feature indices
            for local_idx, gain in tree.feature_importance.items():
                global_idx = int(feature_indices[int(local_idx)])
                aggregated_importance[global_idx] = aggregated_importance.get(global_idx, 0.0) + float(gain)

            # OOB predictions update
            if np.any(oob_mask):
                preds_list = tree.predict(X[oob_mask][:, feature_indices])
                preds = np.array(preds_list, dtype=float)
                oob_vote_sum[oob_mask] += preds
                oob_vote_count[oob_mask] += 1

            # Compute current OOB error (exclude samples with 0 OOB votes)
            valid_mask = oob_vote_count > 0
            if np.any(valid_mask):
                oob_pred = (oob_vote_sum[valid_mask] >= (oob_vote_count[valid_mask] / 2.0)).astype(int)
                y_valid = y[valid_mask]
                oob_error = 1.0 - (np.sum(oob_pred == y_valid) / y_valid.shape[0])
                self.oob_error_curve_.append(float(oob_error))
            else:
                self.oob_error_curve_.append(float('nan'))

        # Normalize feature importances to sum to 1
        total_gain = sum(aggregated_importance.values())
        if total_gain > 0:
            for fi in list(aggregated_importance.keys()):
                aggregated_importance[fi] = aggregated_importance[fi] / total_gain
        self.feature_importances_ = aggregated_importance
        return self

    def predict(self, X: np.ndarray) -> np.ndarray:
        if not self.estimators_:
            raise ValueError("RandomForest is not fitted yet")

        n_samples = X.shape[0]
        votes_sum = np.zeros(n_samples, dtype=float)
        n_trees = len(self.estimators_)

        for tree, feat_idx in zip(self.estimators_, self.feature_indices_per_estimator_):
            preds_list = tree.predict(X[:, feat_idx])
            preds = np.array(preds_list, dtype=float)
            votes_sum += preds

        # Majority vote for binary classification (labels 0/1)
        majority = (votes_sum >= (n_trees / 2.0)).astype(int)
        return majority

# ==== Notebook code cell 17 ====
def accuracy(y_true, y_pred):
    """
    Computes the accuracy of a classification model.

    Parameters:
    ----------
        y_true (numpy array): A numpy array of true labels for each data point.
        y_pred (numpy array): A numpy array of predicted labels for each data point.

    Returns:
    ----------
        float: The accuracy of the model
    """
    y_true = y_true.flatten()
    total_samples = len(y_true)
    correct_predictions = np.sum(y_true == y_pred)
    return (correct_predictions / total_samples)

# ==== Notebook code cell 18 ====
# Convert to numpy
X_train_np = X_train.to_numpy()
y_train_np = y_train.to_numpy()
X_test_np  = X_test.to_numpy()
y_test_np  = y_test.to_numpy()
X_val_np   = X_val.to_numpy()
y_val_np   = y_val.to_numpy()
# Create and train models
# Unpruned tree (for post-pruning):
unpruned_tree = DecisionTree(max_depth=None, min_samples=2, random_state=42)
unpruned_tree.fit(X_train_np, y_train_np)

# Pre-pruned tree at selected depth (example depth=8 as before):
model = DecisionTree(max_depth=8, min_samples=10, random_state=43)
model.fit(X_train_np, y_train_np)

# ==== Notebook code cell 19 ====
y_pred = model.predict(X_test_np)
unpruned_test_pred = unpruned_tree.predict(X_test_np)

# ==== Notebook code cell 20 ====
print(f'  Pre-Pruned Tree accuracy is {accuracy(y_test_np,y_pred)}')
print(f'  Unpruned Tree (before pruning) test accuracy is {accuracy(y_test_np, unpruned_test_pred)}')

# ==== Notebook code cell 20.0 (Reduced Error Post-Pruning) ====
unpruned_val_pred = unpruned_tree.predict(X_val_np)
val_acc_before = accuracy(y_val_np, np.array(unpruned_val_pred))
num_pruned, val_acc_after = reduced_error_prune(
    unpruned_tree, X_train_np, y_train_np, X_val_np, y_val_np
)
unpruned_val_pred_after = unpruned_tree.predict(X_val_np)
unpruned_test_pred_after = unpruned_tree.predict(X_test_np)
test_acc_after = accuracy(y_test_np, np.array(unpruned_test_pred_after))

print(f"Reduced Error Pruning: pruned {num_pruned} nodes")
print(f"Validation accuracy before: {val_acc_before:.3f} after: {val_acc_after:.3f}")
print(f"Test accuracy before: {accuracy(y_test_np, np.array(unpruned_test_pred)):.3f} after: {test_acc_after:.3f}")

# ==== Notebook code cell 20.1 (RandomForest training) ====
# Train a RandomForest on the same data
rf = RandomForest(
    n_estimators=50,
    max_depth=8,
    min_samples=10,
    max_features="sqrt",
    bootstrap=True,
    random_state=42,
)
rf.fit(X_train_np, y_train_np)
rf_pred = rf.predict(X_test_np)
print(f"  RandomForest accuracy is {accuracy(y_test_np, rf_pred)}")

# ==== Notebook code cell 21 ====
def precision(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp + 1e-10)#1e-10 is added to avoid division from zero

def recall(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn + 1e-10)

def f1_score(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return 2 * (p * r) / (p + r + 1e-10)

# ==== Notebook code cell 22 ====
p = precision(y_test_np, np.array(y_pred))
r = recall(y_test_np, np.array(y_pred))
f1 = f1_score(y_test_np, np.array(y_pred))
print(f"DT Precision: {p:.2f}, Recall: {r:.2f}, F1 Score: {f1:.2f}")

# RF metrics
p_rf = precision(y_test_np, np.array(rf_pred))
r_rf = recall(y_test_np, np.array(rf_pred))
f1_rf = f1_score(y_test_np, np.array(rf_pred))
print(f"RF Precision: {p_rf:.2f}, Recall: {r_rf:.2f}, F1 Score: {f1_rf:.2f}")

# ==== Notebook code cell 23 ====
print("Feature importances (DecisionTree):")
for idx, score in model.feature_importance.items():
    print(f"Feature {idx} , {X.columns[idx]} = {round(score, 3)}")

print("Feature importances (RandomForest):")
for idx, score in sorted(rf.feature_importances_.items()):
    print(f"Feature {idx} , {X.columns[idx]} = {round(score, 3)}")

# ==== Notebook code cell 24 ====
train_acc = []
test_acc = []
depth_values = range(1, 21)

for depth in depth_values:
    model = DecisionTree(max_depth=depth, min_samples=2)
    model.fit(X_train_np, y_train_np)
    y_pred_train = model.predict(X_train_np)
    y_pred_test = model.predict(X_val_np)

    train_acc.append(accuracy(y_train_np, y_pred_train))
    test_acc.append(accuracy(y_val_np, y_pred_test))

plt.figure(figsize=(8,5))
plt.plot(depth_values, train_acc, marker='o', label='Train Accuracy')
plt.plot(depth_values, test_acc, marker='s', label='Val Accuracy')
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.title("Effect of max_depth (Pre-pruning Parameter)")
plt.legend()
plt.grid(True)
plt.show()

# ==== Notebook code cell 26 (OOB Error vs Trees) ====
if hasattr(rf, 'oob_error_curve_') and len(rf.oob_error_curve_) > 0:
    plt.figure(figsize=(8,5))
    xs = np.arange(1, len(rf.oob_error_curve_) + 1)
    plt.plot(xs, rf.oob_error_curve_, marker='o')
    plt.xlabel('Number of Trees')
    plt.ylabel('OOB Error')
    plt.title('RandomForest OOB Error vs Number of Trees')
    plt.grid(True)
    plt.show()

# ---------- 2️⃣ Effect of min_samples_split ----------
train_acc2 = []
test_acc2 = []
split_values = [2, 5, 10, 20, 30, 50, 70, 100]

for s in split_values:
    model = DecisionTree(max_depth=None, min_samples=s)
    model.fit(X_train_np, y_train_np)
    y_pred_train = model.predict(X_train_np)
    y_pred_test = model.predict(X_val_np)

    train_acc2.append(accuracy(y_train_np, y_pred_train))
    test_acc2.append(accuracy(y_val_np, y_pred_test))

plt.figure(figsize=(8,5))
plt.plot(split_values, train_acc2, marker='o', label='Train Accuracy')
plt.plot(split_values, test_acc2, marker='s', label='Val Accuracy')
plt.xlabel("Minimum Samples to Split (min_samples)")
plt.ylabel("Accuracy")
plt.title("Effect of min_samples (Pre-pruning Parameter)")
plt.legend()
plt.grid(True)
plt.show()

# ==== Notebook code cell 25 (Accuracy Bar Chart) ====
unpruned_post_test_acc = test_acc_after
prepruned_test_acc = accuracy(y_test_np, np.array(y_pred))
unpruned_pre_test_acc = accuracy(y_test_np, np.array(unpruned_test_pred))

labels = ["Unpruned", "Pre-Pruned", "Post-Pruned"]
scores = [unpruned_pre_test_acc, prepruned_test_acc, unpruned_post_test_acc]
plt.figure(figsize=(7,5))
bars = plt.bar(labels, scores, color=["#999", "#4caf50", "#2196f3"])
plt.ylim(0, 1)
plt.ylabel("Test Accuracy")
plt.title("Decision Tree Accuracies: Unpruned vs Pre-Pruned vs Post-Pruned")
for bar, s in zip(bars, scores):
    plt.text(bar.get_x() + bar.get_width()/2, s + 0.01, f"{s:.2f}", ha='center')
plt.show()
